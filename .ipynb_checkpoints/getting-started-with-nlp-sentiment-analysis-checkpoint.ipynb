{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**----: Problem Statement :----**\n",
    "- **About Practice Problem: Identify the Sentiments**\n",
    "   - Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. Brands can use this data to measure the success of their products in an objective manner. In this challenge, you are provided with tweet data to predict sentiment on electronic products of netizens.\n",
    "\n",
    "  - Sentiment analysis remains one of the key problems that has seen extensive application of natural language processing. This time around, given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc, the task is to identify if the tweets have a negative sentiment towards such companies or products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----: Importing necessary liabraries :----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_profiling as pp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "import re \n",
    "pd.set_option(\"display.max_colwidth\",200)\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**----: Importing Word Net Lemmatizer and stopwords :----**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "stopwords=nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----: Reading Datasets :----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "train_dataset=pd.read_csv(\"../input/train_sentiment.csv\") \n",
    "#training labels\n",
    "label=pd.read_csv(\"../input/train_sentiment.csv\") \n",
    "# testing dataset\n",
    "test_dataset=pd.read_csv(\"../input/test_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**----: Exploring Datasets :----**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the size of the dataset\n",
    "print(\"train_dataset--->\",train_dataset.shape)\n",
    "print(\"test_dataset--->\",test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.drop(\"label\",axis=1)\n",
    "## convertinig each text into lower case\n",
    "train_dataset[\"tweet\"]=train_dataset[\"tweet\"].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the train_dataset to train.\n",
    "train=train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the test_dataset to test.\n",
    "test=test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conctinating the test and train dataset in order to avoid the repetation of cleaning process on this datasets.\n",
    "# we will again split the datasets into train and test after the preprocessing of the datasets.\n",
    "final_dataset=pd.concat([train,test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding a new feature teweet length to the dataset\n",
    "## it shows the length of the tweets\n",
    "final_dataset[\"tweet_length\"]=final_dataset[\"tweet\"].apply(lambda x:len(x)-x.count(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding new column : punct%\n",
    "# punct% shows the percentage of punctuations used in the tweets\n",
    "\n",
    "# function for calculating punctuation percentage in each tweet\n",
    "def punct(text):\n",
    "    count=sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text)-text.count(\" \")),3)*100\n",
    "final_dataset['punct%']=final_dataset[\"tweet\"].apply(lambda x: punct(x))\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove the non-alphabets\n",
    "final_dataset[\"tweet\"]=final_dataset[\"tweet\"].str.replace(\"[^a-z ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising the distribution of tweet length in the datasets\n",
    "fig,ax=plt.subplots(1,3,figsize=(15,5))\n",
    "sns.distplot(final_dataset[\"tweet_length\"],ax=ax[0],color=\"mediumturquoise\")\n",
    "sns.boxplot(final_dataset[\"tweet_length\"],hue=label[\"label\"],ax=ax[1],color=\"turquoise\")\n",
    "sns.violinplot(final_dataset[\"tweet_length\"],hue=label[\"label\"],ax=ax[2],orient=\"v\",color=\"lightgreen\")\n",
    "plt.grid(True)\n",
    "# visualising the distribution of punct% in the datasets\n",
    "fig,ax=plt.subplots(1,3,figsize=(15,5))\n",
    "sns.distplot(final_dataset[\"punct%\"],ax=ax[0],color=\"lightgreen\")\n",
    "sns.boxplot(final_dataset[\"punct%\"],ax=ax[1],color=\"turquoise\")\n",
    "sns.violinplot(final_dataset[\"punct%\"],ax=ax[2],orient=\"v\",color=\"mediumturquoise\")\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting raw tweets into tokenized tweets with no stopwords.\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "# adding new column tweet_nostopwords which consists of tokenized tweets with no stopwords.\n",
    "final_dataset['tweet_nostopwords'] = final_dataset['tweet'].apply(lambda x: clean_text(x.lower()))\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting tokenized tweets into lemmatized tweets.\n",
    "def lemmatizing(tokenized_text):\n",
    "    text =\" \".join([wn.lemmatize(word) for word in tokenized_text])\n",
    "    return text\n",
    "\n",
    "# adding new column tweet_lemmatized which consists of lemmatized tweets.\n",
    "final_dataset['tweet_lemmatized'] = final_dataset['tweet_nostopwords'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Random Forest Model Using Count Vectorizer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imporitng count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the finsl dataset into vector matrix\n",
    "count_vect=CountVectorizer(analyzer=clean_text)\n",
    "X_counts=count_vect.fit_transform(final_dataset[\"tweet_lemmatized\"])\n",
    "print(X_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts # we cannot print the matrix because it is a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to print the matrix we have to convert it to array\n",
    "X_counts_df=pd.DataFrame(X_counts.toarray())\n",
    "#X_counts_df.columns=count_vect.get_feature_names()\n",
    "X_counts_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the tweet length and punct% column to the  vectorized dataset\n",
    "a=pd.DataFrame(np.array(final_dataset['tweet_length']).reshape(-1,1),columns=[\"tweet_length\"])\n",
    "b=pd.DataFrame(np.array(final_dataset['punct%']).reshape(-1,1),columns=[\"punct%\"])\n",
    "final_dataset1= pd.concat([a,b,X_counts_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset for fitting ML model\n",
    "final_dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into train and test as we specified above.\n",
    "final_train=final_dataset1.iloc[0:7920,:]\n",
    "final_test=final_dataset1.iloc[7920:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating object of the model\n",
    "rf = RandomForestClassifier(n_jobs=-1,n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "rf.fit(final_train,label['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on test dataset\n",
    "y_pred=rf.predict(final_test)\n",
    "y_pred=pd.DataFrame(y_pred)\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Random Forest Model Using TF-IDF Vectorizer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(analyzer=clean_text)\n",
    "x_tfidf=tfidf.fit_transform(final_dataset[\"tweet_lemmatized\"])\n",
    "print(x_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in order to print the matrix we have to convert it to array\n",
    "x_tfidf_df=pd.DataFrame(x_tfidf.toarray())\n",
    "#x_tfidf_df.columns=tfidf.get_feature_names()\n",
    "x_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the tweet length and punct% column to the  vectorized dataset\n",
    "a=pd.DataFrame(np.array(final_dataset['tweet_length']).reshape(-1,1))\n",
    "b=pd.DataFrame(np.array(final_dataset['punct%']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset1= pd.concat([a,b,x_tfidf_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset for fitting ML model\n",
    "final_dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into train and test as we specified above.\n",
    "final_train=final_dataset1.iloc[0:7920,:]\n",
    "final_test=final_dataset1.iloc[7920:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating object of the model\n",
    "rf = RandomForestClassifier(n_jobs=-1,n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "rf.fit(final_train,label['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on test dataset\n",
    "y_pred=rf.predict(final_test)\n",
    "y_pred=pd.DataFrame(y_pred)\n",
    "y_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 427197,
     "sourceId": 813168,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29841,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
